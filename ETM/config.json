{
  "model_version": "0.1",
  "description": "base ETM model - 24.9.2020",
  "random_seed": 1984,
  "raw_data_path": {
    "AVRAHAMI-PC": "C:\\Users\\avrahami\\Documents\\Private\\IDC\\influencer_influenced_project\\topic_models\\\\pickles_for_etm_model"
  },
  "data_path": {
    "AVRAHAMI-PC": "C:\\Users\\avrahami\\Documents\\Private\\IDC\\influencer_influenced_project\\topic_models\\ETM_data\\intuview"
  },
  "emb_path": {
    "AVRAHAMI-PC": "C:\\Users\\avrahami\\Documents\\Private\\IDC\\influencer_influenced_project\\topic_models\\embeddings_model\\tweets_cbow_300"
  },
  "saving_data_path": {
    "AVRAHAMI-PC": "C:\\Users\\avrahami\\Documents\\Private\\IDC\\influencer_influenced_project\\topic_models\\ETM_data"
  },
  "saving_models_path": {
    "AVRAHAMI-PC": "C:\\Users\\avrahami\\Documents\\Private\\IDC\\influencer_influenced_project\\topic_models\\ETM_results"
  },
  //name of corpus - can be 20ng or wikipedia or intuview
  "dataset": "intuview",
  //whether or not to prepare data for modeling. In case False - all matrices are expected to be found in "saving_data_path"
  "prepare_data": "False",
  //input batch size for training
  "batch_size": 10,
  "data_prep_params":{
    "min_df":  0.01,
    "max_df": 0.7,
    "stop_words_f_name": "stops_arabic.txt",
    //whether to save the pre-process obj. Most relevant when we want to run a "predict" function later
    "save_model": "True",
    //relevant only in case "save_model" is set to True, or when we run 'predict' option. SHOULD NOT include the .p or .pickle at the end
    "saving_model_f_name": "pre_process_obj_24_9_2020",
    "raw_data_path": {
      "AVRAHAMI-PC": "C:\\Users\\avrahami\\Documents\\Private\\IDC\\influencer_influenced_project\\topic_models\\pickles_for_etm_model"
    }
  },
  //model-related arguments
  "model_params": {
    //number of topics, default: 50
    "num_topics": 50,
    //dimension of rho
    "rho_size": 300,
    //dimension of embeddings
    "emb_size": 300,
    //dimension of hidden space of q(theta)
    "t_hidden_size": 800,
    //'tanh, softplus, relu, rrelu, leakyrelu, elu, selu, glu)
    "theta_act": "relu",
    //whether to fix rho or train it
    "train_embeddings": "False"
  },
  //optimization-related arguments
  "optimization_params": {
    //learning rate
    "lr": 0.005,
    //divide learning rate by this...
    "lr_factor": 4.0,
    //number of epochs to train...150 for 20ng 100 for others
    "epochs": 100,
    //train, eval or predict model
    "mode": "eval",
    //choice of optimizer
    "optimizer": "adam",
    //dropout rate on encoder
    "enc_drop": 0.0,
    //gradient clipping
    "clip": 0.0,
    //number of bad hits allowed
    "nonmono": 10,
    //some l2 regularization
    "wdecay": 1.2e-6,
    //whether to anneal the learning rate or not
    "anneal_lr": 0,
    //normalize the bows or not
    "bow_norm": 1
  },
  //evaluation, visualization, and logging-related arguments
  "evaluation_params": {
    //number of words for topic vizualize
    "num_words": 10,
    //when to log training
    "log_interval": 2,
    //when to visualize results
    "visualize_every": 10,
    //input batch size for evaluation
    "eval_batch_size": 1000,
    //the name of the ckpt to eval from, including the explicit name of the model
    "load_from": "C:\\Users\\avrahami\\Documents\\Private\\IDC\\influencer_influenced_project\\topic_models\\ETM_results\\etm_intuview_K_50_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_10_RhoSize_300_trainEmbeddings_False",
    //whether to compute topic coherence or not
    "tc": 0,
    //whether to compute topic diversity or not
    "td": 0
  }
}
