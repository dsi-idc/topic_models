{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETM model demo\n",
    "---------------\n",
    "This notebook intends to show an exmaple of how to run the ETM model (for arabic).\n",
    "\n",
    "We show here how to run fit / predict / eval processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports, functions and other configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import torch\n",
    "import numpy as np \n",
    "import os\n",
    "from torch import optim\n",
    "from etm import ETM\n",
    "from utils import prepare_embedding_matrix\n",
    "import commentjson\n",
    "from data_prep.arabic_twitter import ArabicTwitterPreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_optimizer():\n",
    "    if config_dict['optimization_params']['optimizer'] == 'adam':\n",
    "        selected_optimizer = optim.Adam(etm_model.parameters(), lr=config_dict['optimization_params']['lr'],\n",
    "                                        weight_decay=config_dict['optimization_params']['wdecay'])\n",
    "    elif config_dict['optimization_params']['optimizer'] == 'adagrad':\n",
    "        selected_optimizer = optim.Adagrad(etm_model.parameters(), lr=config_dict['optimization_params']['lr'],\n",
    "                                           weight_decay=config_dict['optimization_params']['wdecay'])\n",
    "    elif config_dict['optimization_params']['optimizer'] == 'adadelta':\n",
    "        selected_optimizer = optim.Adadelta(etm_model.parameters(), lr=config_dict['optimization_params']['lr'],\n",
    "                                            weight_decay=config_dict['optimization_params']['wdecay'])\n",
    "    elif config_dict['optimization_params']['optimizer'] == 'rmsprop':\n",
    "        selected_optimizer = optim.RMSprop(etm_model.parameters(), lr=config_dict['optimization_params']['lr'],\n",
    "                                           weight_decay=config_dict['optimization_params']['wdecay'])\n",
    "    elif config_dict['optimization_params']['optimizer'] == 'asgd':\n",
    "        selected_optimizer = optim.ASGD(etm_model.parameters(), lr=config_dict['optimization_params']['lr'],\n",
    "                                        t0=0, lambd=0., weight_decay=config_dict['optimization_params']['wdecay'])\n",
    "    else:\n",
    "        print('Defaulting to vanilla SGD')\n",
    "        selected_optimizer = optim.SGD(etm_model.parameters(), lr=config_dict['optimization_params']['lr'])\n",
    "    return selected_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine = 'AVRAHAMI-PC'\n",
    "config_dict = commentjson.load(open('config.json'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "np.random.seed(config_dict['random_seed'])\n",
    "torch.manual_seed(config_dict['random_seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(config_dict['random_seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Load and prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " initial vocabulary size: 8897\n",
      "  vocabulary size after removing stopwords from list: 8583\n",
      "  vocabulary after removing stopwords: 8583\n",
      " vocabulary after removing words not in train: 8480\n",
      " number of documents (train): 283 [this should be equal to 283]\n",
      " number of documents (test): 33 [this should be equal to 33]\n",
      " number of documents (valid): 17 [this should be equal to 17]\n",
      " len(words_tr):  153962\n",
      " len(words_ts):  8121\n",
      " len(words_ts_h1):  4051\n",
      " len(words_ts_h2):  4070\n",
      " len(words_va):  8801\n",
      " len(np.unique(doc_indices_tr)): 283 [this should be 283]\n",
      " len(np.unique(doc_indices_ts)): 33 [this should be 33]\n",
      " len(np.unique(doc_indices_ts_h1)): 33 [this should be 33]\n",
      " len(np.unique(doc_indices_ts_h2)): 33 [this should be 33]\n",
      " len(np.unique(doc_indices_va)): 17 [this should be 17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready !! All data has been saved in C:\\Users\\avrahami\\Documents\\Private\\IDC\\influencer_influenced_project\\topic_models\\ETM_data\\intuview\\\n"
     ]
    }
   ],
   "source": [
    "if eval(config_dict['prepare_data']):\n",
    "    preprocess_obj = ArabicTwitterPreProcess(config_dict=config_dict, machine=machine)\n",
    "    #preprocess_obj._calculate_stats(data_path=config_dict['raw_data_path'][machine])\n",
    "    preprocess_obj.fit_transform(data_path=config_dict['raw_data_path'][machine], verbose=True)\n",
    "    if eval(config_dict['data_prep_params']['save_model']):\n",
    "        preprocess_model_f_name = config_dict['data_prep_params']['saving_model_f_name'] + '.p'\n",
    "        preprocess_obj.save_obj(f_name=preprocess_model_f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, train, valid, test = data.get_data(os.path.join(config_dict['data_path'][machine]))\n",
    "vocab_size = len(vocab)\n",
    "config_dict['vocab_size'] = vocab_size\n",
    "\n",
    "# 1. training data\n",
    "train_tokens = train['tokens']\n",
    "train_counts = train['counts']\n",
    "config_dict['num_docs_train'] = len(train_tokens)\n",
    "\n",
    "# 2. dev set\n",
    "valid_tokens = valid['tokens']\n",
    "valid_counts = valid['counts']\n",
    "config_dict['num_docs_valid'] = len(valid_tokens)\n",
    "\n",
    "# 3. test data\n",
    "test_tokens = test['tokens']\n",
    "test_counts = test['counts']\n",
    "config_dict['num_docs_test'] = len(test_tokens)\n",
    "test_1_tokens = test['tokens_1']\n",
    "test_1_counts = test['counts_1']\n",
    "config_dict['num_docs_test_1'] = len(test_1_tokens)\n",
    "test_2_tokens = test['tokens_2']\n",
    "test_2_counts = test['counts_2']\n",
    "config_dict['num_docs_test_2'] = len(test_2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "Training an Embedded Topic Model on INTUVIEW\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n"
     ]
    }
   ],
   "source": [
    "# embeddings handeling (internal/external)\n",
    "embeddings = None\n",
    "# in case we gave as input the embeddings file to be used as pre-trained model\n",
    "if not eval(config_dict['model_params']['train_embeddings']):\n",
    "    embeddings = prepare_embedding_matrix(emb_data_path=config_dict['emb_path'][machine],\n",
    "                                          emb_size=config_dict['model_params']['emb_size'], vocab=vocab,\n",
    "                                          random_seed=config_dict['random_seed'])\n",
    "    # updating the required values after the function returned the embbeddings\n",
    "    embeddings = torch.from_numpy(embeddings).to(device)\n",
    "    config_dict['embeddings_dim'] = embeddings.size()\n",
    "print('=*'*100)\n",
    "print(f\"Training an Embedded Topic Model on {config_dict['dataset'].upper()}\")\n",
    "print('=*'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define checkpoint\n",
    "if not os.path.exists(config_dict['saving_models_path'][machine]):\n",
    "    os.makedirs(config_dict['saving_models_path'][machine])\n",
    "\n",
    "if config_dict['optimization_params']['mode'] == 'eval':\n",
    "    ckpt = config_dict['evaluation_params']['load_from']\n",
    "else:\n",
    "    ckpt = \\\n",
    "        os.path.join(config_dict['saving_models_path'][machine],\n",
    "                     'etm_{}_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_'\n",
    "                     'trainEmbeddings_{}'.format(config_dict['dataset'], config_dict['model_params']['num_topics'],\n",
    "                                                 config_dict['model_params']['t_hidden_size'],\n",
    "                                                 config_dict['optimization_params']['optimizer'],\n",
    "                                                 config_dict['optimization_params']['clip'],\n",
    "                                                 config_dict['model_params']['theta_act'],\n",
    "                                                 config_dict['optimization_params']['lr'],\n",
    "                                                 config_dict['batch_size'],\n",
    "                                                 config_dict['model_params']['rho_size'],\n",
    "                                                 config_dict['model_params']['train_embeddings']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.0)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=8480, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=30, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=8480, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=800, out_features=30, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=800, out_features=30, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "etm_model = ETM(config_dict=config_dict, machine=machine, embeddings=embeddings)\n",
    "print('model: {}'.format(etm_model))\n",
    "optimizer = _set_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fitting a new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** use this option in case you want to fit a new model rather than using an existing one ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 5846.66 .. NELBO: 5846.68\n",
      "Epoch: 1 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 4778.43 .. NELBO: 4778.47\n",
      "Epoch: 1 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 4099.5 .. NELBO: 4099.55\n",
      "Epoch: 1 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 3753.78 .. NELBO: 3753.84\n",
      "Epoch: 1 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 3946.9 .. NELBO: 3946.98\n",
      "Epoch: 1 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 5045.53 .. NELBO: 5045.7\n",
      "Epoch: 1 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 0.4 .. Rec_loss: 4812.84 .. NELBO: 4813.24\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.4 .. Rec_loss: 4812.84 .. NELBO: 4813.24\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 2.72 .. Rec_loss: 3402.37 .. NELBO: 3405.09\n",
      "Epoch: 2 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 2.43 .. Rec_loss: 2981.07 .. NELBO: 2983.5\n",
      "Epoch: 2 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 2.1 .. Rec_loss: 2997.33 .. NELBO: 2999.43\n",
      "Epoch: 2 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 1.83 .. Rec_loss: 3192.25 .. NELBO: 3194.08\n",
      "Epoch: 2 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 1.61 .. Rec_loss: 3143.66 .. NELBO: 3145.27\n",
      "Epoch: 2 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 1.51 .. Rec_loss: 4605.89 .. NELBO: 4607.4\n",
      "Epoch: 2 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 1.32 .. Rec_loss: 4265.07 .. NELBO: 4266.39\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 1.32 .. Rec_loss: 4265.07 .. NELBO: 4266.39\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 3767.93 .. NELBO: 3768.47\n",
      "Epoch: 3 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 3776.78 .. NELBO: 3777.28\n",
      "Epoch: 3 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 1.34 .. Rec_loss: 3413.14 .. NELBO: 3414.48\n",
      "Epoch: 3 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 3.19 .. Rec_loss: 4404.01 .. NELBO: 4407.2\n",
      "Epoch: 3 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 4.23 .. Rec_loss: 4247.05 .. NELBO: 4251.28\n",
      "Epoch: 3 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 4.08 .. Rec_loss: 4403.6 .. NELBO: 4407.68\n",
      "Epoch: 3 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 3.67 .. Rec_loss: 4080.97 .. NELBO: 4084.64\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 3.67 .. Rec_loss: 4080.97 .. NELBO: 4084.64\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 5.33 .. Rec_loss: 4688.82 .. NELBO: 4694.15\n",
      "Epoch: 4 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 3.85 .. Rec_loss: 3594.78 .. NELBO: 3598.63\n",
      "Epoch: 4 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 4.97 .. Rec_loss: 4034.85 .. NELBO: 4039.82\n",
      "Epoch: 4 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 4.68 .. Rec_loss: 3827.61 .. NELBO: 3832.29\n",
      "Epoch: 4 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 4.58 .. Rec_loss: 3626.4 .. NELBO: 3630.98\n",
      "Epoch: 4 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 5.82 .. Rec_loss: 4302.62 .. NELBO: 4308.44\n",
      "Epoch: 4 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 5.48 .. Rec_loss: 4138.65 .. NELBO: 4144.13\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 5.48 .. Rec_loss: 4138.65 .. NELBO: 4144.13\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 6.64 .. Rec_loss: 7327.85 .. NELBO: 7334.49\n",
      "Epoch: 5 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 5.33 .. Rec_loss: 5841.97 .. NELBO: 5847.3\n",
      "Epoch: 5 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 4.77 .. Rec_loss: 5066.07 .. NELBO: 5070.84\n",
      "Epoch: 5 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 4.98 .. Rec_loss: 4588.65 .. NELBO: 4593.63\n",
      "Epoch: 5 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 6.03 .. Rec_loss: 4120.23 .. NELBO: 4126.26\n",
      "Epoch: 5 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 8.41 .. Rec_loss: 4140.03 .. NELBO: 4148.44\n",
      "Epoch: 5 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 23.45 .. Rec_loss: 5140.54 .. NELBO: 5163.99\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 23.45 .. Rec_loss: 5140.54 .. NELBO: 5163.99\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 9.85 .. Rec_loss: 3350.62 .. NELBO: 3360.47\n",
      "Epoch: 6 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 10.91 .. Rec_loss: 5484.01 .. NELBO: 5494.92\n",
      "Epoch: 6 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 9.44 .. Rec_loss: 4606.96 .. NELBO: 4616.4\n",
      "Epoch: 6 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 8.73 .. Rec_loss: 4225.15 .. NELBO: 4233.88\n",
      "Epoch: 6 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 8.04 .. Rec_loss: 3883.53 .. NELBO: 3891.57\n",
      "Epoch: 6 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 7.94 .. Rec_loss: 4130.27 .. NELBO: 4138.21\n",
      "Epoch: 6 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 7.59 .. Rec_loss: 4002.57 .. NELBO: 4010.16\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 7.59 .. Rec_loss: 4002.57 .. NELBO: 4010.16\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 6.38 .. Rec_loss: 4074.6 .. NELBO: 4080.98\n",
      "Epoch: 7 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 6.58 .. Rec_loss: 5120.89 .. NELBO: 5127.47\n",
      "Epoch: 7 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 7.51 .. Rec_loss: 4946.31 .. NELBO: 4953.82\n",
      "Epoch: 7 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 7.35 .. Rec_loss: 4676.19 .. NELBO: 4683.54\n",
      "Epoch: 7 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 7.33 .. Rec_loss: 4337.5 .. NELBO: 4344.83\n",
      "Epoch: 7 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 6.69 .. Rec_loss: 4063.01 .. NELBO: 4069.7\n",
      "Epoch: 7 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 6.95 .. Rec_loss: 3991.36 .. NELBO: 3998.31\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 6.95 .. Rec_loss: 3991.36 .. NELBO: 3998.31\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 2.03 .. Rec_loss: 2744.35 .. NELBO: 2746.38\n",
      "Epoch: 8 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 1.99 .. Rec_loss: 2610.45 .. NELBO: 2612.44\n",
      "Epoch: 8 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 2631.19 .. NELBO: 2633.92\n",
      "Epoch: 8 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 5.53 .. Rec_loss: 3813.61 .. NELBO: 3819.14\n",
      "Epoch: 8 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 7.97 .. Rec_loss: 4548.37 .. NELBO: 4556.34\n",
      "Epoch: 8 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 7.32 .. Rec_loss: 4277.28 .. NELBO: 4284.6\n",
      "Epoch: 8 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 6.47 .. Rec_loss: 3948.42 .. NELBO: 3954.89\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 6.47 .. Rec_loss: 3948.42 .. NELBO: 3954.89\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 3.39 .. Rec_loss: 4636.56 .. NELBO: 4639.95\n",
      "Epoch: 9 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 4.64 .. Rec_loss: 4088.96 .. NELBO: 4093.6\n",
      "Epoch: 9 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 4.32 .. Rec_loss: 3603.43 .. NELBO: 3607.75\n",
      "Epoch: 9 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 4.87 .. Rec_loss: 3606.52 .. NELBO: 3611.39\n",
      "Epoch: 9 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 5.24 .. Rec_loss: 4024.21 .. NELBO: 4029.45\n",
      "Epoch: 9 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 5.21 .. Rec_loss: 3936.68 .. NELBO: 3941.89\n",
      "Epoch: 9 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 7.37 .. Rec_loss: 4854.34 .. NELBO: 4861.71\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 7.37 .. Rec_loss: 4854.34 .. NELBO: 4861.71\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 2230.43 .. NELBO: 2233.17\n",
      "Epoch: 10 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 2.44 .. Rec_loss: 2601.74 .. NELBO: 2604.18\n",
      "Epoch: 10 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 3.33 .. Rec_loss: 4023.85 .. NELBO: 4027.18\n",
      "Epoch: 10 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 4.59 .. Rec_loss: 4455.9 .. NELBO: 4460.49\n",
      "Epoch: 10 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 4.46 .. Rec_loss: 4025.12 .. NELBO: 4029.58\n",
      "Epoch: 10 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 4.62 .. Rec_loss: 4021.73 .. NELBO: 4026.35\n",
      "Epoch: 10 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 4.53 .. Rec_loss: 3912.05 .. NELBO: 3916.58\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 4.53 .. Rec_loss: 3912.05 .. NELBO: 3916.58\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 4.46 .. Rec_loss: 2714.22 .. NELBO: 2718.68\n",
      "Epoch: 11 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 5.19 .. Rec_loss: 3733.65 .. NELBO: 3738.84\n",
      "Epoch: 11 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 5.58 .. Rec_loss: 3803.18 .. NELBO: 3808.76\n",
      "Epoch: 11 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 5.82 .. Rec_loss: 3982.11 .. NELBO: 3987.93\n",
      "Epoch: 11 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 5.69 .. Rec_loss: 4260.13 .. NELBO: 4265.82\n",
      "Epoch: 11 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 5.73 .. Rec_loss: 4197.61 .. NELBO: 4203.34\n",
      "Epoch: 11 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 5.27 .. Rec_loss: 3967.94 .. NELBO: 3973.21\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 5.27 .. Rec_loss: 3967.94 .. NELBO: 3973.21\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 3.29 .. Rec_loss: 3265.03 .. NELBO: 3268.32\n",
      "Epoch: 12 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 3.88 .. Rec_loss: 3529.29 .. NELBO: 3533.17\n",
      "Epoch: 12 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 4.59 .. Rec_loss: 3606.07 .. NELBO: 3610.66\n",
      "Epoch: 12 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 5.75 .. Rec_loss: 3800.62 .. NELBO: 3806.37\n",
      "Epoch: 12 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 6.59 .. Rec_loss: 3691.68 .. NELBO: 3698.27\n",
      "Epoch: 12 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 6.2 .. Rec_loss: 4109.3 .. NELBO: 4115.5\n",
      "Epoch: 12 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 7.11 .. Rec_loss: 4576.31 .. NELBO: 4583.42\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 7.11 .. Rec_loss: 4576.31 .. NELBO: 4583.42\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 3.33 .. Rec_loss: 3176.17 .. NELBO: 3179.5\n",
      "Epoch: 13 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 3.17 .. Rec_loss: 3031.24 .. NELBO: 3034.41\n",
      "Epoch: 13 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 3.57 .. Rec_loss: 3137.54 .. NELBO: 3141.11\n",
      "Epoch: 13 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 4.0 .. Rec_loss: 3857.27 .. NELBO: 3861.27\n",
      "Epoch: 13 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 3.81 .. Rec_loss: 3630.48 .. NELBO: 3634.29\n",
      "Epoch: 13 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 4.21 .. Rec_loss: 3756.52 .. NELBO: 3760.73\n",
      "Epoch: 13 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 4.41 .. Rec_loss: 3854.79 .. NELBO: 3859.2\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 4.41 .. Rec_loss: 3854.79 .. NELBO: 3859.2\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 6.96 .. Rec_loss: 4557.27 .. NELBO: 4564.23\n",
      "Epoch: 14 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 5.93 .. Rec_loss: 3840.7 .. NELBO: 3846.63\n",
      "Epoch: 14 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 6.61 .. Rec_loss: 4203.01 .. NELBO: 4209.62\n",
      "Epoch: 14 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 6.05 .. Rec_loss: 3726.92 .. NELBO: 3732.97\n",
      "Epoch: 14 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 5.63 .. Rec_loss: 3686.45 .. NELBO: 3692.08\n",
      "Epoch: 14 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 5.74 .. Rec_loss: 4134.0 .. NELBO: 4139.74\n",
      "Epoch: 14 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 5.47 .. Rec_loss: 3898.51 .. NELBO: 3903.98\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 5.47 .. Rec_loss: 3898.51 .. NELBO: 3903.98\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 7.76 .. Rec_loss: 6673.73 .. NELBO: 6681.49\n",
      "Epoch: 15 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 6.51 .. Rec_loss: 4976.98 .. NELBO: 4983.49\n",
      "Epoch: 15 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 6.55 .. Rec_loss: 4434.33 .. NELBO: 4440.88\n",
      "Epoch: 15 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 6.37 .. Rec_loss: 4021.66 .. NELBO: 4028.03\n",
      "Epoch: 15 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 7.72 .. Rec_loss: 4454.78 .. NELBO: 4462.5\n",
      "Epoch: 15 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 7.17 .. Rec_loss: 4096.4 .. NELBO: 4103.57\n",
      "Epoch: 15 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 6.71 .. Rec_loss: 4138.76 .. NELBO: 4145.47\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 6.71 .. Rec_loss: 4138.76 .. NELBO: 4145.47\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 6.2 .. Rec_loss: 4115.99 .. NELBO: 4122.19\n",
      "Epoch: 16 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 6.38 .. Rec_loss: 3878.17 .. NELBO: 3884.55\n",
      "Epoch: 16 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 6.25 .. Rec_loss: 4339.84 .. NELBO: 4346.09\n",
      "Epoch: 16 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 5.81 .. Rec_loss: 4017.79 .. NELBO: 4023.6\n",
      "Epoch: 16 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 5.81 .. Rec_loss: 3946.39 .. NELBO: 3952.2\n",
      "Epoch: 16 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 5.54 .. Rec_loss: 3721.27 .. NELBO: 3726.81\n",
      "Epoch: 16 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 6.28 .. Rec_loss: 3901.43 .. NELBO: 3907.71\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 6.28 .. Rec_loss: 3901.43 .. NELBO: 3907.71\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 11.83 .. Rec_loss: 6775.45 .. NELBO: 6787.28\n",
      "Epoch: 17 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 10.73 .. Rec_loss: 5785.6 .. NELBO: 5796.33\n",
      "Epoch: 17 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 9.73 .. Rec_loss: 5126.7 .. NELBO: 5136.43\n",
      "Epoch: 17 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 8.8 .. Rec_loss: 4718.95 .. NELBO: 4727.75\n",
      "Epoch: 17 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 8.25 .. Rec_loss: 4229.52 .. NELBO: 4237.77\n",
      "Epoch: 17 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 4029.24 .. NELBO: 4037.12\n",
      "Epoch: 17 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 7.79 .. Rec_loss: 3845.58 .. NELBO: 3853.37\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 7.79 .. Rec_loss: 3845.58 .. NELBO: 3853.37\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 7.27 .. Rec_loss: 3831.35 .. NELBO: 3838.62\n",
      "Epoch: 18 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 8.11 .. Rec_loss: 4441.98 .. NELBO: 4450.09\n",
      "Epoch: 18 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 8.12 .. Rec_loss: 4050.11 .. NELBO: 4058.23\n",
      "Epoch: 18 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 7.69 .. Rec_loss: 4078.31 .. NELBO: 4086.0\n",
      "Epoch: 18 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 7.49 .. Rec_loss: 4315.57 .. NELBO: 4323.06\n",
      "Epoch: 18 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 7.36 .. Rec_loss: 4051.14 .. NELBO: 4058.5\n",
      "Epoch: 18 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 7.45 .. Rec_loss: 3929.33 .. NELBO: 3936.78\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 7.45 .. Rec_loss: 3929.33 .. NELBO: 3936.78\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 9.61 .. Rec_loss: 4543.32 .. NELBO: 4552.93\n",
      "Epoch: 19 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 8.79 .. Rec_loss: 3786.79 .. NELBO: 3795.58\n",
      "Epoch: 19 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 8.61 .. Rec_loss: 3933.82 .. NELBO: 3942.43\n",
      "Epoch: 19 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 8.82 .. Rec_loss: 3952.8 .. NELBO: 3961.62\n",
      "Epoch: 19 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 8.83 .. Rec_loss: 3835.56 .. NELBO: 3844.39\n",
      "Epoch: 19 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 8.71 .. Rec_loss: 4158.79 .. NELBO: 4167.5\n",
      "Epoch: 19 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 8.36 .. Rec_loss: 3899.12 .. NELBO: 3907.48\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 8.36 .. Rec_loss: 3899.12 .. NELBO: 3907.48\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 9.08 .. Rec_loss: 4878.98 .. NELBO: 4888.06\n",
      "Epoch: 20 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 8.56 .. Rec_loss: 4218.44 .. NELBO: 4227.0\n",
      "Epoch: 20 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 7.92 .. Rec_loss: 3724.63 .. NELBO: 3732.55\n",
      "Epoch: 20 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 7.77 .. Rec_loss: 3509.99 .. NELBO: 3517.76\n",
      "Epoch: 20 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 8.32 .. Rec_loss: 3627.61 .. NELBO: 3635.93\n",
      "Epoch: 20 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 8.56 .. Rec_loss: 3585.94 .. NELBO: 3594.5\n",
      "Epoch: 20 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 10.13 .. Rec_loss: 6601.51 .. NELBO: 6611.64\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 10.13 .. Rec_loss: 6601.51 .. NELBO: 6611.64\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 11.64 .. Rec_loss: 4643.86 .. NELBO: 4655.5\n",
      "Epoch: 21 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 10.05 .. Rec_loss: 3607.11 .. NELBO: 3617.16\n",
      "Epoch: 21 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 10.01 .. Rec_loss: 3481.46 .. NELBO: 3491.47\n",
      "Epoch: 21 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 10.5 .. Rec_loss: 4280.87 .. NELBO: 4291.37\n",
      "Epoch: 21 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 10.09 .. Rec_loss: 3978.65 .. NELBO: 3988.74\n",
      "Epoch: 21 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 9.69 .. Rec_loss: 3849.19 .. NELBO: 3858.88\n",
      "Epoch: 21 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 10.26 .. Rec_loss: 4017.84 .. NELBO: 4028.1\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 10.26 .. Rec_loss: 4017.84 .. NELBO: 4028.1\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 11.96 .. Rec_loss: 3454.07 .. NELBO: 3466.03\n",
      "Epoch: 22 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 11.8 .. Rec_loss: 2873.61 .. NELBO: 2885.41\n",
      "Epoch: 22 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 11.26 .. Rec_loss: 3342.88 .. NELBO: 3354.14\n",
      "Epoch: 22 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 11.27 .. Rec_loss: 4102.48 .. NELBO: 4113.75\n",
      "Epoch: 22 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 10.49 .. Rec_loss: 3849.23 .. NELBO: 3859.72\n",
      "Epoch: 22 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 10.71 .. Rec_loss: 4080.91 .. NELBO: 4091.62\n",
      "Epoch: 22 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 10.59 .. Rec_loss: 3850.79 .. NELBO: 3861.38\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 10.59 .. Rec_loss: 3850.79 .. NELBO: 3861.38\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 12.98 .. Rec_loss: 5168.92 .. NELBO: 5181.9\n",
      "Epoch: 23 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 11.51 .. Rec_loss: 3968.55 .. NELBO: 3980.06\n",
      "Epoch: 23 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 10.48 .. Rec_loss: 3522.05 .. NELBO: 3532.53\n",
      "Epoch: 23 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 10.27 .. Rec_loss: 3753.82 .. NELBO: 3764.09\n",
      "Epoch: 23 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 10.42 .. Rec_loss: 3717.8 .. NELBO: 3728.22\n",
      "Epoch: 23 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 9.89 .. Rec_loss: 4177.78 .. NELBO: 4187.67\n",
      "Epoch: 23 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 9.35 .. Rec_loss: 4096.57 .. NELBO: 4105.92\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 9.35 .. Rec_loss: 4096.57 .. NELBO: 4105.92\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 6.95 .. Rec_loss: 2866.16 .. NELBO: 2873.11\n",
      "Epoch: 24 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 2938.08 .. NELBO: 2944.15\n",
      "Epoch: 24 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 6.99 .. Rec_loss: 3511.73 .. NELBO: 3518.72\n",
      "Epoch: 24 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 7.97 .. Rec_loss: 4155.74 .. NELBO: 4163.71\n",
      "Epoch: 24 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 8.4 .. Rec_loss: 4424.02 .. NELBO: 4432.42\n",
      "Epoch: 24 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 8.14 .. Rec_loss: 4082.44 .. NELBO: 4090.58\n",
      "Epoch: 24 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 8.17 .. Rec_loss: 3851.88 .. NELBO: 3860.05\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 8.17 .. Rec_loss: 3851.88 .. NELBO: 3860.05\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 9.81 .. Rec_loss: 5998.12 .. NELBO: 6007.93\n",
      "Epoch: 25 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 9.18 .. Rec_loss: 4500.3 .. NELBO: 4509.48\n",
      "Epoch: 25 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 9.43 .. Rec_loss: 4110.96 .. NELBO: 4120.39\n",
      "Epoch: 25 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 10.23 .. Rec_loss: 4082.49 .. NELBO: 4092.72\n",
      "Epoch: 25 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 10.5 .. Rec_loss: 4001.59 .. NELBO: 4012.09\n",
      "Epoch: 25 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 10.08 .. Rec_loss: 3865.61 .. NELBO: 3875.69\n",
      "Epoch: 25 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 10.23 .. Rec_loss: 3839.89 .. NELBO: 3850.12\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 10.23 .. Rec_loss: 3839.89 .. NELBO: 3850.12\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 7.66 .. Rec_loss: 4052.08 .. NELBO: 4059.74\n",
      "Epoch: 26 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 7.05 .. Rec_loss: 3479.7 .. NELBO: 3486.75\n",
      "Epoch: 26 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 7.28 .. Rec_loss: 3233.99 .. NELBO: 3241.27\n",
      "Epoch: 26 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 7.79 .. Rec_loss: 3282.2 .. NELBO: 3289.99\n",
      "Epoch: 26 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 8.45 .. Rec_loss: 4070.66 .. NELBO: 4079.11\n",
      "Epoch: 26 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 8.38 .. Rec_loss: 3866.21 .. NELBO: 3874.59\n",
      "Epoch: 26 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 8.78 .. Rec_loss: 3977.84 .. NELBO: 3986.62\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 8.78 .. Rec_loss: 3977.84 .. NELBO: 3986.62\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 7.25 .. Rec_loss: 2545.44 .. NELBO: 2552.69\n",
      "Epoch: 27 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 8.68 .. Rec_loss: 4069.84 .. NELBO: 4078.52\n",
      "Epoch: 27 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 9.27 .. Rec_loss: 3819.15 .. NELBO: 3828.42\n",
      "Epoch: 27 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 9.42 .. Rec_loss: 3803.93 .. NELBO: 3813.35\n",
      "Epoch: 27 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 9.08 .. Rec_loss: 3664.46 .. NELBO: 3673.54\n",
      "Epoch: 27 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 8.64 .. Rec_loss: 3669.82 .. NELBO: 3678.46\n",
      "Epoch: 27 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 8.61 .. Rec_loss: 3825.91 .. NELBO: 3834.52\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 8.61 .. Rec_loss: 3825.91 .. NELBO: 3834.52\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 8.86 .. Rec_loss: 4662.65 .. NELBO: 4671.51\n",
      "Epoch: 28 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 9.83 .. Rec_loss: 3933.33 .. NELBO: 3943.16\n",
      "Epoch: 28 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 9.51 .. Rec_loss: 3536.66 .. NELBO: 3546.17\n",
      "Epoch: 28 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 9.72 .. Rec_loss: 3863.28 .. NELBO: 3873.0\n",
      "Epoch: 28 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 10.03 .. Rec_loss: 3804.32 .. NELBO: 3814.35\n",
      "Epoch: 28 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 9.39 .. Rec_loss: 3513.27 .. NELBO: 3522.66\n",
      "Epoch: 28 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 9.36 .. Rec_loss: 3801.72 .. NELBO: 3811.08\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 9.36 .. Rec_loss: 3801.72 .. NELBO: 3811.08\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 9.06 .. Rec_loss: 3362.33 .. NELBO: 3371.39\n",
      "Epoch: 29 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 9.52 .. Rec_loss: 3290.07 .. NELBO: 3299.59\n",
      "Epoch: 29 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 9.71 .. Rec_loss: 3954.73 .. NELBO: 3964.44\n",
      "Epoch: 29 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 9.59 .. Rec_loss: 3732.59 .. NELBO: 3742.18\n",
      "Epoch: 29 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 9.46 .. Rec_loss: 3518.49 .. NELBO: 3527.95\n",
      "Epoch: 29 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 9.88 .. Rec_loss: 3548.93 .. NELBO: 3558.81\n",
      "Epoch: 29 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 10.87 .. Rec_loss: 6636.52 .. NELBO: 6647.39\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 10.87 .. Rec_loss: 6636.52 .. NELBO: 6647.39\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 9.68 .. Rec_loss: 4769.63 .. NELBO: 4779.31\n",
      "Epoch: 30 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 10.16 .. Rec_loss: 4844.51 .. NELBO: 4854.67\n",
      "Epoch: 30 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 9.92 .. Rec_loss: 4297.49 .. NELBO: 4307.41\n",
      "Epoch: 30 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 10.08 .. Rec_loss: 3939.93 .. NELBO: 3950.01\n",
      "Epoch: 30 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 10.56 .. Rec_loss: 4335.81 .. NELBO: 4346.37\n",
      "Epoch: 30 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 10.71 .. Rec_loss: 4065.48 .. NELBO: 4076.19\n",
      "Epoch: 30 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 10.48 .. Rec_loss: 3831.98 .. NELBO: 3842.46\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 10.48 .. Rec_loss: 3831.98 .. NELBO: 3842.46\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 14.13 .. Rec_loss: 5057.39 .. NELBO: 5071.52\n",
      "Epoch: 31 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 11.5 .. Rec_loss: 3864.66 .. NELBO: 3876.16\n",
      "Epoch: 31 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 10.7 .. Rec_loss: 4336.39 .. NELBO: 4347.09\n",
      "Epoch: 31 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 10.39 .. Rec_loss: 3931.1 .. NELBO: 3941.49\n",
      "Epoch: 31 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 10.5 .. Rec_loss: 3879.22 .. NELBO: 3889.72\n",
      "Epoch: 31 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 11.02 .. Rec_loss: 4138.32 .. NELBO: 4149.34\n",
      "Epoch: 31 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 10.74 .. Rec_loss: 3887.79 .. NELBO: 3898.53\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 10.74 .. Rec_loss: 3887.79 .. NELBO: 3898.53\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c46523e69075>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m etm_model.fit(optimizer=optimizer, train_tokens=train_tokens, train_counts=train_counts,\n\u001b[0;32m      2\u001b[0m                       \u001b[0mtest_1_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_1_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_1_counts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_1_counts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_2_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_2_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                       test_2_counts=test_2_counts, vocab=vocab, ckpt=ckpt)\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Private\\IDC\\influencer_influenced_project\\topic_models\\git_repository\\ETM\\etm.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, optimizer, train_tokens, train_counts, test_1_tokens, test_1_counts, test_2_tokens, test_2_counts, vocab, ckpt)\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'optimization_params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epochs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m             cur_train_loss = self.train_single_epoch(epoch=epoch, optimizer=optimizer, train_tokens=train_tokens,\n\u001b[1;32m--> 393\u001b[1;33m                                                      train_counts=train_counts)\n\u001b[0m\u001b[0;32m    394\u001b[0m             \u001b[0mall_train_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_train_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m             val_ppl = self.evaluate(test_1_tokens=test_1_tokens, test_1_counts=test_1_counts,\n",
      "\u001b[1;32m~\\Documents\\Private\\IDC\\influencer_influenced_project\\topic_models\\git_repository\\ETM\\etm.py\u001b[0m in \u001b[0;36mtrain_single_epoch\u001b[1;34m(self, epoch, optimizer, train_tokens, train_counts)\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \u001b[0mnormalized_data_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[0mrecon_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkld_theta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalized_data_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecon_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkld_theta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[0mtotal_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Private\\IDC\\influencer_influenced_project\\topic_models\\git_repository\\ETM\\etm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, bows, normalized_bows, theta, aggregate)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;31m# get \\beta (the distribution over the vocab of each topic)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_beta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;31m# get prediction loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Private\\IDC\\influencer_influenced_project\\topic_models\\git_repository\\ETM\\etm.py\u001b[0m in \u001b[0;36mget_beta\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[0mlogit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malphas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrho\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;31m# softmax over vocab dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m         \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1261\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1263\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1264\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "etm_model.fit(optimizer=optimizer, train_tokens=train_tokens, train_counts=train_counts,\n",
    "                      test_1_tokens=test_1_tokens, test_1_counts=test_1_counts, test_2_tokens=test_2_tokens,\n",
    "                      test_2_counts=test_2_counts, vocab=vocab, ckpt=ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction to new data using an existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_model_f_name = config_dict['data_prep_params']['saving_model_f_name'] + '.p'\n",
    "preprocess_obj = ArabicTwitterPreProcess.load_obj(f_path=config_dict['saving_models_path'][machine],\n",
    "                                                  f_name=preprocess_model_f_name)\n",
    "bow_new_docs_tokens, bow_new_docs_counts = preprocess_obj.transform(data_path=config_dict['raw_data_path'][machine])\n",
    "data_batch = data.get_batch(bow_new_docs_tokens, bow_new_docs_counts,\n",
    "                            torch.tensor(list(range(len(bow_new_docs_tokens)))), vocab_size, device)\n",
    "sums = data_batch.sum(1).unsqueeze(1)\n",
    "normalized_data_batch = data_batch / sums\n",
    "new_docs_prediction = etm_model.predict_proba(bow_tokens=normalized_data_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evluating an existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing model quality before training...\n",
      "\n",
      "\n",
      "Topic 0: ['Asphalt', 'palestine', '2000', 'Occupiers', 'For fear', 'Passed', 'launch', 'The only one', 'the list']\n",
      "Topic 1: ['Expensive', 'Turkey', 'Crown', 'And prevent', 'And made it', 'father', 'Damage', 'The beloved', 'virtually']\n",
      "Topic 2: ['exactly', 'Affectionate', 'He is talking', 'And sell', 'The darkness', 'And Islamic', 'Mahrez', 'Mine', 'Their role']\n",
      "Topic 3: ['Monkey', 'the video', 'Ka', 'Yaqyum', 'pregnant', 'Way', 'The conquest', 'Leave', 'Kinship']\n",
      "Topic 4: ['Saudis', 'To Islam', 'Habit', 'time', 'Salary', 'Occupiers', 'Helps', 'Her land', 'For the Zionists']\n",
      "Topic 5: ['Ambiance', 'Most', 'The Caravan', 'Dirtier', 'For Muslims', 'Capital', 'The kind', 'Tighten', 'Emirates']\n",
      "Topic 6: ['minus', 'most', 'Haniyeh', 'You know', 'Thinking', 'Curses you', 'Taste', 'Nation', 'Erase']\n",
      "Topic 7: ['I understand', 'Serving', 'Islamic', 'Resting place', 'Hah', 'Your Tweet', 'wife', 'Believe me', 'Cherish']\n",
      "Topic 8: ['For the Zionists', 'To destroy', 'Palestine', '24', 'Asak', 'And its wint', 'God', 'Apostasy', 'And the end']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5dee4b751240>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0metm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m etm_model.print_words_per_topic(words_amount=config_dict['evaluation_params']['num_words'],\n\u001b[1;32m---> 10\u001b[1;33m                                 vocab=vocab, lang='en')\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# get document completion perplexities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Private\\IDC\\influencer_influenced_project\\topic_models\\git_repository\\ETM\\etm.py\u001b[0m in \u001b[0;36mprint_words_per_topic\u001b[1;34m(self, words_amount, vocab, lang)\u001b[0m\n\u001b[0;32m    481\u001b[0m                 \u001b[0mtopic_words_en\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mcur_word\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopic_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m                     \u001b[0mtopic_words_en\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Topic {}: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_words_en\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\googletrans\\client.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[0morigin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;31m# this code will be updated when the format is changed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\googletrans\\client.py\u001b[0m in \u001b[0;36m_translate\u001b[1;34m(self, text, dest, src, override)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRANSLATE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pick_service_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\httpx\\_client.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url, params, headers, cookies, auth, allow_redirects, timeout)\u001b[0m\n\u001b[0;32m    761\u001b[0m             \u001b[0mauth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_redirects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m             \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    764\u001b[0m         )\n\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\httpx\\_client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, data, files, json, params, headers, cookies, auth, allow_redirects, timeout)\u001b[0m\n\u001b[0;32m    599\u001b[0m         )\n\u001b[0;32m    600\u001b[0m         return self.send(\n\u001b[1;32m--> 601\u001b[1;33m             \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_redirects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m         )\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\httpx\\_client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, auth, allow_redirects, timeout)\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         response = self.send_handling_redirects(\n\u001b[1;32m--> 621\u001b[1;33m             \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_redirects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    622\u001b[0m         )\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\httpx\\_client.py\u001b[0m in \u001b[0;36msend_handling_redirects\u001b[1;34m(self, request, auth, timeout, allow_redirects, history)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m             response = self.send_handling_auth(\n\u001b[1;32m--> 648\u001b[1;33m                 \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m             )\n\u001b[0;32m    650\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\httpx\\_client.py\u001b[0m in \u001b[0;36msend_handling_auth\u001b[1;34m(self, request, history, auth, timeout)\u001b[0m\n\u001b[0;32m    682\u001b[0m         \u001b[0mrequest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauth_flow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 684\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_single_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_response_body\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\httpx\\_client.py\u001b[0m in \u001b[0;36msend_single_request\u001b[1;34m(self, request, timeout)\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m                 \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m             )\n\u001b[0;32m    721\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                 response = connection.request(\n\u001b[1;32m--> 153\u001b[1;33m                     \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m                 )\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mNewConnectionRequired\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\httpcore\\_sync\\connection.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;34m\"connection.request method=%r url=%r headers=%r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         )\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_open_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTimeoutDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mSyncSocketStream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\httpcore\\_sync\\http11.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mreason_phrase\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         ) = self._receive_response(timeout)\n\u001b[0m\u001b[0;32m     63\u001b[0m         stream = SyncByteStream(\n\u001b[0;32m     64\u001b[0m             \u001b[0miterator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_receive_response_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\httpcore\\_sync\\http11.py\u001b[0m in \u001b[0;36m_receive_response\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[0mevent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\httpcore\\_sync\\http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh11_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreceive_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\httpcore\\_backends\\sync.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, n, timeout)\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTimeoutDict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                     \u001b[1;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                     self.__class__)\n\u001b[1;32m-> 1037\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    911\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 913\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    914\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# eval option\n",
    "with open(ckpt, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print('Visualizing model quality before training...')\n",
    "etm_model.eval()\n",
    "etm_model.print_words_per_topic(words_amount=config_dict['evaluation_params']['num_words'],\n",
    "                                vocab=vocab, lang='en')\n",
    "with torch.no_grad():\n",
    "    # get document completion perplexities\n",
    "    test_ppl = model.evaluate(source='test', test_1_tokens=test_1_tokens, test_1_counts=test_1_counts,\n",
    "                              test_2_tokens=test_2_tokens, test_2_counts=test_2_counts,\n",
    "                              train_tokens=train_tokens, vocab=vocab, tc=config_dict['evaluation_params']['tc'],\n",
    "                              td=config_dict['evaluation_params']['td'])\n",
    "\n",
    "    # get most used topics\n",
    "    indices = torch.tensor(range(config_dict['num_docs_train']))\n",
    "    indices = torch.split(indices, config_dict['batch_size'])\n",
    "    thetaAvg = torch.zeros(1, config_dict['model_params']['num_topics']).to(device)\n",
    "    thetaWeightedAvg = torch.zeros(1, config_dict['model_params']['num_topics']).to(device)\n",
    "    cnt = 0\n",
    "    for idx, ind in enumerate(indices):\n",
    "        data_batch = data.get_batch(train_tokens, train_counts, ind, config_dict['vocab_size'], device)\n",
    "        sums = data_batch.sum(1).unsqueeze(1)\n",
    "        cnt += sums.sum(0).squeeze().cpu().numpy()\n",
    "        if config_dict['optimization_params']['bow_norm']:\n",
    "            normalized_data_batch = data_batch / sums\n",
    "        else:\n",
    "            normalized_data_batch = data_batch\n",
    "        theta, _ = model.get_theta(normalized_data_batch)\n",
    "        thetaAvg += theta.sum(0).unsqueeze(0) / config_dict['num_docs_train']\n",
    "        weighed_theta = sums * theta\n",
    "        thetaWeightedAvg += weighed_theta.sum(0).unsqueeze(0)\n",
    "        if idx % 100 == 0 and idx > 0:\n",
    "            print('batch: {}/{}'.format(idx, len(indices)))\n",
    "    thetaWeightedAvg = thetaWeightedAvg.squeeze().cpu().numpy() / cnt\n",
    "    print('\\nThe 10 most used topics are {}'.format(thetaWeightedAvg.argsort()[::-1][:10]))\n",
    "\n",
    "    # show topics\n",
    "    etm_model.print_words_per_topic(words_amount=config_dict['evaluation_params']['num_words'], vocab=vocab)\n",
    "    etm_model.print_words_per_topic(words_amount=config_dict['evaluation_params']['num_words'],\n",
    "                                    vocab=vocab, lang='en')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
