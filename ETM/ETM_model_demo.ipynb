{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETM model demo\n",
    "---------------\n",
    "This notebook intends to show an exmaple of how to run the ETM model (for arabic).\n",
    "\n",
    "We show here how to run fit / predict / eval processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports, functions and other configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import torch\n",
    "import numpy as np \n",
    "import os\n",
    "from torch import optim\n",
    "from etm import ETM\n",
    "from utils import prepare_embedding_matrix\n",
    "import commentjson\n",
    "from data_prep.arabic_twitter import ArabicTwitterPreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_optimizer():\n",
    "    if config_dict['optimization_params']['optimizer'] == 'adam':\n",
    "        selected_optimizer = optim.Adam(etm_model.parameters(), lr=config_dict['optimization_params']['lr'],\n",
    "                                        weight_decay=config_dict['optimization_params']['wdecay'])\n",
    "    elif config_dict['optimization_params']['optimizer'] == 'adagrad':\n",
    "        selected_optimizer = optim.Adagrad(etm_model.parameters(), lr=config_dict['optimization_params']['lr'],\n",
    "                                           weight_decay=config_dict['optimization_params']['wdecay'])\n",
    "    elif config_dict['optimization_params']['optimizer'] == 'adadelta':\n",
    "        selected_optimizer = optim.Adadelta(etm_model.parameters(), lr=config_dict['optimization_params']['lr'],\n",
    "                                            weight_decay=config_dict['optimization_params']['wdecay'])\n",
    "    elif config_dict['optimization_params']['optimizer'] == 'rmsprop':\n",
    "        selected_optimizer = optim.RMSprop(etm_model.parameters(), lr=config_dict['optimization_params']['lr'],\n",
    "                                           weight_decay=config_dict['optimization_params']['wdecay'])\n",
    "    elif config_dict['optimization_params']['optimizer'] == 'asgd':\n",
    "        selected_optimizer = optim.ASGD(etm_model.parameters(), lr=config_dict['optimization_params']['lr'],\n",
    "                                        t0=0, lambd=0., weight_decay=config_dict['optimization_params']['wdecay'])\n",
    "    else:\n",
    "        print('Defaulting to vanilla SGD')\n",
    "        selected_optimizer = optim.SGD(etm_model.parameters(), lr=config_dict['optimization_params']['lr'])\n",
    "    return selected_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine = 'AVRAHAMI-PC'\n",
    "config_dict = commentjson.load(open('config.json'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "np.random.seed(config_dict['random_seed'])\n",
    "torch.manual_seed(config_dict['random_seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(config_dict['random_seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Load and prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " initial vocabulary size: 8897\n",
      "  vocabulary size after removing stopwords from list: 8583\n",
      "  vocabulary after removing stopwords: 8583\n",
      " vocabulary after removing words not in train: 8480\n",
      " number of documents (train): 283 [this should be equal to 283]\n",
      " number of documents (test): 33 [this should be equal to 33]\n",
      " number of documents (valid): 17 [this should be equal to 17]\n",
      " len(words_tr):  153962\n",
      " len(words_ts):  8121\n",
      " len(words_ts_h1):  4051\n",
      " len(words_ts_h2):  4070\n",
      " len(words_va):  8801\n",
      " len(np.unique(doc_indices_tr)): 283 [this should be 283]\n",
      " len(np.unique(doc_indices_ts)): 33 [this should be 33]\n",
      " len(np.unique(doc_indices_ts_h1)): 33 [this should be 33]\n",
      " len(np.unique(doc_indices_ts_h2)): 33 [this should be 33]\n",
      " len(np.unique(doc_indices_va)): 17 [this should be 17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready !! All data has been saved in C:\\Users\\avrahami\\Documents\\Private\\IDC\\influencer_influenced_project\\topic_models\\ETM_data\\intuview\\\n"
     ]
    }
   ],
   "source": [
    "if eval(config_dict['prepare_data']):\n",
    "    preprocess_obj = ArabicTwitterPreProcess(config_dict=config_dict, machine=machine)\n",
    "    #preprocess_obj._calculate_stats(data_path=config_dict['raw_data_path'][machine])\n",
    "    preprocess_obj.fit_transform(data_path=config_dict['raw_data_path'][machine], verbose=True)\n",
    "    if eval(config_dict['data_prep_params']['save_model']):\n",
    "        preprocess_model_f_name = config_dict['data_prep_params']['saving_model_f_name'] + '.p'\n",
    "        preprocess_obj.save_obj(f_name=preprocess_model_f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, train, valid, test = data.get_data(os.path.join(config_dict['data_path'][machine]))\n",
    "vocab_size = len(vocab)\n",
    "config_dict['vocab_size'] = vocab_size\n",
    "\n",
    "# 1. training data\n",
    "train_tokens = train['tokens']\n",
    "train_counts = train['counts']\n",
    "config_dict['num_docs_train'] = len(train_tokens)\n",
    "\n",
    "# 2. dev set\n",
    "valid_tokens = valid['tokens']\n",
    "valid_counts = valid['counts']\n",
    "config_dict['num_docs_valid'] = len(valid_tokens)\n",
    "\n",
    "# 3. test data\n",
    "test_tokens = test['tokens']\n",
    "test_counts = test['counts']\n",
    "config_dict['num_docs_test'] = len(test_tokens)\n",
    "test_1_tokens = test['tokens_1']\n",
    "test_1_counts = test['counts_1']\n",
    "config_dict['num_docs_test_1'] = len(test_1_tokens)\n",
    "test_2_tokens = test['tokens_2']\n",
    "test_2_counts = test['counts_2']\n",
    "config_dict['num_docs_test_2'] = len(test_2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "Training an Embedded Topic Model on INTUVIEW\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n"
     ]
    }
   ],
   "source": [
    "# embeddings handeling (internal/external)\n",
    "embeddings = None\n",
    "# in case we gave as input the embeddings file to be used as pre-trained model\n",
    "if not eval(config_dict['model_params']['train_embeddings']):\n",
    "    embeddings = prepare_embedding_matrix(emb_data_path=config_dict['emb_path'][machine],\n",
    "                                          emb_size=config_dict['model_params']['emb_size'], vocab=vocab,\n",
    "                                          random_seed=config_dict['random_seed'])\n",
    "    # updating the required values after the function returned the embbeddings\n",
    "    embeddings = torch.from_numpy(embeddings).to(device)\n",
    "    config_dict['embeddings_dim'] = embeddings.size()\n",
    "print('=*'*100)\n",
    "print(f\"Training an Embedded Topic Model on {config_dict['dataset'].upper()}\")\n",
    "print('=*'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define checkpoint\n",
    "if not os.path.exists(config_dict['saving_models_path'][machine]):\n",
    "    os.makedirs(config_dict['saving_models_path'][machine])\n",
    "\n",
    "if config_dict['optimization_params']['mode'] == 'eval':\n",
    "    ckpt = config_dict['evaluation_params']['load_from']\n",
    "else:\n",
    "    ckpt = \\\n",
    "        os.path.join(config_dict['saving_models_path'][machine],\n",
    "                     'etm_{}_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_'\n",
    "                     'trainEmbeddings_{}'.format(config_dict['dataset'], config_dict['model_params']['num_topics'],\n",
    "                                                 config_dict['model_params']['t_hidden_size'],\n",
    "                                                 config_dict['optimization_params']['optimizer'],\n",
    "                                                 config_dict['optimization_params']['clip'],\n",
    "                                                 config_dict['model_params']['theta_act'],\n",
    "                                                 config_dict['optimization_params']['lr'],\n",
    "                                                 config_dict['batch_size'],\n",
    "                                                 config_dict['model_params']['rho_size'],\n",
    "                                                 config_dict['model_params']['train_embeddings']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.0)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=8480, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=30, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=8480, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=800, out_features=30, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=800, out_features=30, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "etm_model = ETM(config_dict=config_dict, machine=machine, embeddings=embeddings)\n",
    "print('model: {}'.format(etm_model))\n",
    "optimizer = _set_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fitting a new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** use this option in case you want to fit a new model rather than using an existing one ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 5846.3 .. NELBO: 5846.32\n",
      "Epoch: 1 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 4777.97 .. NELBO: 4778.0\n",
      "Epoch: 1 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 4098.88 .. NELBO: 4098.93\n",
      "Epoch: 1 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 3752.86 .. NELBO: 3752.93\n",
      "Epoch: 1 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 3945.27 .. NELBO: 3945.39\n",
      "Epoch: 1 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 5041.03 .. NELBO: 5041.3\n",
      "Epoch: 1 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 0.66 .. Rec_loss: 4808.43 .. NELBO: 4809.09\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.66 .. Rec_loss: 4808.43 .. NELBO: 4809.09\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 3.41 .. Rec_loss: 3397.47 .. NELBO: 3400.88\n",
      "Epoch: 2 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 2.77 .. Rec_loss: 2977.51 .. NELBO: 2980.28\n",
      "Epoch: 2 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 2.24 .. Rec_loss: 2994.55 .. NELBO: 2996.79\n",
      "Epoch: 2 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 1.87 .. Rec_loss: 3189.01 .. NELBO: 3190.88\n",
      "Epoch: 2 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 1.6 .. Rec_loss: 3141.06 .. NELBO: 3142.66\n",
      "Epoch: 2 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 1.41 .. Rec_loss: 4603.72 .. NELBO: 4605.13\n",
      "Epoch: 2 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 1.23 .. Rec_loss: 4263.11 .. NELBO: 4264.34\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 1.23 .. Rec_loss: 4263.11 .. NELBO: 4264.34\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 3770.33 .. NELBO: 3770.51\n",
      "Epoch: 3 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 3779.41 .. NELBO: 3779.61\n",
      "Epoch: 3 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 0.76 .. Rec_loss: 3415.67 .. NELBO: 3416.43\n",
      "Epoch: 3 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 2.17 .. Rec_loss: 4414.44 .. NELBO: 4416.61\n",
      "Epoch: 3 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 2.92 .. Rec_loss: 4255.62 .. NELBO: 4258.54\n",
      "Epoch: 3 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 3.9 .. Rec_loss: 4409.63 .. NELBO: 4413.53\n",
      "Epoch: 3 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 3.71 .. Rec_loss: 4086.13 .. NELBO: 4089.84\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 3.71 .. Rec_loss: 4086.13 .. NELBO: 4089.84\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 9.29 .. Rec_loss: 4680.01 .. NELBO: 4689.3\n",
      "Epoch: 4 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 7.35 .. Rec_loss: 3588.53 .. NELBO: 3595.88\n",
      "Epoch: 4 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 7.43 .. Rec_loss: 4032.05 .. NELBO: 4039.48\n",
      "Epoch: 4 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 6.3 .. Rec_loss: 3826.3 .. NELBO: 3832.6\n",
      "Epoch: 4 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 5.49 .. Rec_loss: 3625.88 .. NELBO: 3631.37\n",
      "Epoch: 4 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 5.32 .. Rec_loss: 4310.04 .. NELBO: 4315.36\n",
      "Epoch: 4 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 5.19 .. Rec_loss: 4145.38 .. NELBO: 4150.57\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 5.19 .. Rec_loss: 4145.38 .. NELBO: 4150.57\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 9.63 .. Rec_loss: 7349.69 .. NELBO: 7359.32\n",
      "Epoch: 5 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 8.99 .. Rec_loss: 5857.53 .. NELBO: 5866.52\n",
      "Epoch: 5 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 8.0 .. Rec_loss: 5078.14 .. NELBO: 5086.14\n",
      "Epoch: 5 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 7.07 .. Rec_loss: 4600.31 .. NELBO: 4607.38\n",
      "Epoch: 5 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 6.27 .. Rec_loss: 4129.79 .. NELBO: 4136.06\n",
      "Epoch: 5 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 5.98 .. Rec_loss: 4154.21 .. NELBO: 4160.19\n",
      "Epoch: 5 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 6.59 .. Rec_loss: 5163.0 .. NELBO: 5169.59\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 6.59 .. Rec_loss: 5163.0 .. NELBO: 5169.59\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 10.53 .. Rec_loss: 3370.24 .. NELBO: 3380.77\n",
      "Epoch: 6 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 12.29 .. Rec_loss: 5517.74 .. NELBO: 5530.03\n",
      "Epoch: 6 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 11.77 .. Rec_loss: 4633.23 .. NELBO: 4645.0\n",
      "Epoch: 6 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 11.49 .. Rec_loss: 4247.99 .. NELBO: 4259.48\n",
      "Epoch: 6 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 10.97 .. Rec_loss: 3903.79 .. NELBO: 3914.76\n",
      "Epoch: 6 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 12.81 .. Rec_loss: 4145.06 .. NELBO: 4157.87\n",
      "Epoch: 6 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 13.0 .. Rec_loss: 4015.94 .. NELBO: 4028.94\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 13.0 .. Rec_loss: 4015.94 .. NELBO: 4028.94\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 12.39 .. Rec_loss: 4080.36 .. NELBO: 4092.75\n",
      "Epoch: 7 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 12.3 .. Rec_loss: 5117.56 .. NELBO: 5129.86\n",
      "Epoch: 7 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 12.77 .. Rec_loss: 4943.46 .. NELBO: 4956.23\n",
      "Epoch: 7 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 11.46 .. Rec_loss: 4674.24 .. NELBO: 4685.7\n",
      "Epoch: 7 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 10.66 .. Rec_loss: 4336.44 .. NELBO: 4347.1\n",
      "Epoch: 7 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 9.83 .. Rec_loss: 4063.34 .. NELBO: 4073.17\n",
      "Epoch: 7 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 10.43 .. Rec_loss: 3990.64 .. NELBO: 4001.07\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 10.43 .. Rec_loss: 3990.64 .. NELBO: 4001.07\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 7.18 .. Rec_loss: 2738.96 .. NELBO: 2746.14\n",
      "Epoch: 8 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 7.15 .. Rec_loss: 2606.0 .. NELBO: 2613.15\n",
      "Epoch: 8 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 7.75 .. Rec_loss: 2625.93 .. NELBO: 2633.68\n",
      "Epoch: 8 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 12.24 .. Rec_loss: 3873.78 .. NELBO: 3886.02\n",
      "Epoch: 8 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 14.46 .. Rec_loss: 4605.08 .. NELBO: 4619.54\n",
      "Epoch: 8 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 13.14 .. Rec_loss: 4326.71 .. NELBO: 4339.85\n",
      "Epoch: 8 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 11.85 .. Rec_loss: 3992.47 .. NELBO: 4004.32\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 11.85 .. Rec_loss: 3992.47 .. NELBO: 4004.32\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 6.2 .. Rec_loss: 4649.49 .. NELBO: 4655.69\n",
      "Epoch: 9 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 6.22 .. Rec_loss: 4100.41 .. NELBO: 4106.63\n",
      "Epoch: 9 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 6.34 .. Rec_loss: 3613.03 .. NELBO: 3619.37\n",
      "Epoch: 9 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 6.82 .. Rec_loss: 3618.76 .. NELBO: 3625.58\n",
      "Epoch: 9 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 6.79 .. Rec_loss: 4035.58 .. NELBO: 4042.37\n",
      "Epoch: 9 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 6.43 .. Rec_loss: 3948.06 .. NELBO: 3954.49\n",
      "Epoch: 9 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 8.86 .. Rec_loss: 4852.78 .. NELBO: 4861.64\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 8.86 .. Rec_loss: 4852.78 .. NELBO: 4861.64\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 6.84 .. Rec_loss: 2228.64 .. NELBO: 2235.48\n",
      "Epoch: 10 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 6.89 .. Rec_loss: 2599.76 .. NELBO: 2606.65\n",
      "Epoch: 10 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 8.86 .. Rec_loss: 4032.0 .. NELBO: 4040.86\n",
      "Epoch: 10 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 9.57 .. Rec_loss: 4474.4 .. NELBO: 4483.97\n",
      "Epoch: 10 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 8.69 .. Rec_loss: 4040.72 .. NELBO: 4049.41\n",
      "Epoch: 10 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 8.14 .. Rec_loss: 4040.46 .. NELBO: 4048.6\n",
      "Epoch: 10 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 7.62 .. Rec_loss: 3928.33 .. NELBO: 3935.95\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 7.62 .. Rec_loss: 3928.33 .. NELBO: 3935.95\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 5.38 .. Rec_loss: 2712.4 .. NELBO: 2717.78\n",
      "Epoch: 11 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 6.5 .. Rec_loss: 3736.55 .. NELBO: 3743.05\n",
      "Epoch: 11 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 6.75 .. Rec_loss: 3810.17 .. NELBO: 3816.92\n",
      "Epoch: 11 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 6.73 .. Rec_loss: 3986.61 .. NELBO: 3993.34\n",
      "Epoch: 11 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 6.66 .. Rec_loss: 4270.43 .. NELBO: 4277.09\n",
      "Epoch: 11 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 7.13 .. Rec_loss: 4204.82 .. NELBO: 4211.95\n",
      "Epoch: 11 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 7.13 .. Rec_loss: 3974.11 .. NELBO: 3981.24\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 7.13 .. Rec_loss: 3974.11 .. NELBO: 3981.24\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 9.99 .. Rec_loss: 3261.17 .. NELBO: 3271.16\n",
      "Epoch: 12 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 11.79 .. Rec_loss: 3528.26 .. NELBO: 3540.05\n",
      "Epoch: 12 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 13.06 .. Rec_loss: 3605.76 .. NELBO: 3618.82\n",
      "Epoch: 12 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 13.26 .. Rec_loss: 3796.6 .. NELBO: 3809.86\n",
      "Epoch: 12 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 12.42 .. Rec_loss: 3686.67 .. NELBO: 3699.09\n",
      "Epoch: 12 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 11.73 .. Rec_loss: 4104.66 .. NELBO: 4116.39\n",
      "Epoch: 12 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 11.1 .. Rec_loss: 4564.43 .. NELBO: 4575.53\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 11.1 .. Rec_loss: 4564.43 .. NELBO: 4575.53\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 6.01 .. Rec_loss: 3168.4 .. NELBO: 3174.41\n",
      "Epoch: 13 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 6.67 .. Rec_loss: 3025.52 .. NELBO: 3032.19\n",
      "Epoch: 13 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 6.8 .. Rec_loss: 3132.58 .. NELBO: 3139.38\n",
      "Epoch: 13 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 6.62 .. Rec_loss: 3854.4 .. NELBO: 3861.02\n",
      "Epoch: 13 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 6.19 .. Rec_loss: 3628.47 .. NELBO: 3634.66\n",
      "Epoch: 13 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 6.61 .. Rec_loss: 3754.3 .. NELBO: 3760.91\n",
      "Epoch: 13 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 6.83 .. Rec_loss: 3851.66 .. NELBO: 3858.49\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 6.83 .. Rec_loss: 3851.66 .. NELBO: 3858.49\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 12.69 .. Rec_loss: 4548.44 .. NELBO: 4561.13\n",
      "Epoch: 14 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 9.2 .. Rec_loss: 3834.23 .. NELBO: 3843.43\n",
      "Epoch: 14 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 9.24 .. Rec_loss: 4200.39 .. NELBO: 4209.63\n",
      "Epoch: 14 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 8.29 .. Rec_loss: 3727.16 .. NELBO: 3735.45\n",
      "Epoch: 14 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 7.53 .. Rec_loss: 3688.95 .. NELBO: 3696.48\n",
      "Epoch: 14 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 8.06 .. Rec_loss: 4131.95 .. NELBO: 4140.01\n",
      "Epoch: 14 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 7.39 .. Rec_loss: 3897.51 .. NELBO: 3904.9\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 7.39 .. Rec_loss: 3897.51 .. NELBO: 3904.9\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 12.29 .. Rec_loss: 6706.08 .. NELBO: 6718.37\n",
      "Epoch: 15 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 8.92 .. Rec_loss: 5000.27 .. NELBO: 5009.19\n",
      "Epoch: 15 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 7.52 .. Rec_loss: 4452.84 .. NELBO: 4460.36\n",
      "Epoch: 15 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 6.91 .. Rec_loss: 4036.86 .. NELBO: 4043.77\n",
      "Epoch: 15 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 8.19 .. Rec_loss: 4473.85 .. NELBO: 4482.04\n",
      "Epoch: 15 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 7.74 .. Rec_loss: 4113.04 .. NELBO: 4120.78\n",
      "Epoch: 15 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 7.48 .. Rec_loss: 4153.93 .. NELBO: 4161.41\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 7.48 .. Rec_loss: 4153.93 .. NELBO: 4161.41\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 5.68 .. Rec_loss: 4120.59 .. NELBO: 4126.27\n",
      "Epoch: 16 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 5.18 .. Rec_loss: 3881.34 .. NELBO: 3886.52\n",
      "Epoch: 16 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 5.09 .. Rec_loss: 4342.55 .. NELBO: 4347.64\n",
      "Epoch: 16 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 4.67 .. Rec_loss: 4021.12 .. NELBO: 4025.79\n",
      "Epoch: 16 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 4.75 .. Rec_loss: 3949.22 .. NELBO: 3953.97\n",
      "Epoch: 16 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 4.69 .. Rec_loss: 3723.57 .. NELBO: 3728.26\n",
      "Epoch: 16 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 5.34 .. Rec_loss: 3905.45 .. NELBO: 3910.79\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 5.34 .. Rec_loss: 3905.45 .. NELBO: 3910.79\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 14.78 .. Rec_loss: 6798.13 .. NELBO: 6812.91\n",
      "Epoch: 17 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 12.97 .. Rec_loss: 5810.67 .. NELBO: 5823.64\n",
      "Epoch: 17 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 12.18 .. Rec_loss: 5143.75 .. NELBO: 5155.93\n",
      "Epoch: 17 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 10.49 .. Rec_loss: 4735.44 .. NELBO: 4745.93\n",
      "Epoch: 17 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 9.6 .. Rec_loss: 4244.57 .. NELBO: 4254.17\n",
      "Epoch: 17 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 8.86 .. Rec_loss: 4041.15 .. NELBO: 4050.01\n",
      "Epoch: 17 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 8.47 .. Rec_loss: 3856.58 .. NELBO: 3865.05\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 8.47 .. Rec_loss: 3856.58 .. NELBO: 3865.05\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 7.44 .. Rec_loss: 3841.0 .. NELBO: 3848.44\n",
      "Epoch: 18 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 4462.98 .. NELBO: 4470.82\n",
      "Epoch: 18 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 7.73 .. Rec_loss: 4067.98 .. NELBO: 4075.71\n",
      "Epoch: 18 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 7.53 .. Rec_loss: 4094.55 .. NELBO: 4102.08\n",
      "Epoch: 18 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 7.07 .. Rec_loss: 4333.35 .. NELBO: 4340.42\n",
      "Epoch: 18 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 6.93 .. Rec_loss: 4067.17 .. NELBO: 4074.1\n",
      "Epoch: 18 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 6.46 .. Rec_loss: 3944.07 .. NELBO: 3950.53\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 6.46 .. Rec_loss: 3944.07 .. NELBO: 3950.53\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 7.29 .. Rec_loss: 4559.28 .. NELBO: 4566.57\n",
      "Epoch: 19 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 6.74 .. Rec_loss: 3802.85 .. NELBO: 3809.59\n",
      "Epoch: 19 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 6.99 .. Rec_loss: 3944.26 .. NELBO: 3951.25\n",
      "Epoch: 19 .. batch: 8/15 .. LR: 0.005 .. KL_theta: 7.54 .. Rec_loss: 3965.87 .. NELBO: 3973.41\n",
      "Epoch: 19 .. batch: 10/15 .. LR: 0.005 .. KL_theta: 7.48 .. Rec_loss: 3845.86 .. NELBO: 3853.34\n",
      "Epoch: 19 .. batch: 12/15 .. LR: 0.005 .. KL_theta: 7.38 .. Rec_loss: 4169.35 .. NELBO: 4176.73\n",
      "Epoch: 19 .. batch: 14/15 .. LR: 0.005 .. KL_theta: 7.12 .. Rec_loss: 3908.98 .. NELBO: 3916.1\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 7.12 .. Rec_loss: 3908.98 .. NELBO: 3916.1\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 2/15 .. LR: 0.005 .. KL_theta: 9.11 .. Rec_loss: 4888.73 .. NELBO: 4897.84\n",
      "Epoch: 20 .. batch: 4/15 .. LR: 0.005 .. KL_theta: 8.99 .. Rec_loss: 4227.54 .. NELBO: 4236.53\n",
      "Epoch: 20 .. batch: 6/15 .. LR: 0.005 .. KL_theta: 8.12 .. Rec_loss: 3732.26 .. NELBO: 3740.38\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c46523e69075>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m etm_model.fit(optimizer=optimizer, train_tokens=train_tokens, train_counts=train_counts,\n\u001b[0;32m      2\u001b[0m                       \u001b[0mtest_1_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_1_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_1_counts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_1_counts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_2_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_2_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                       test_2_counts=test_2_counts, vocab=vocab, ckpt=ckpt)\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Private\\IDC\\influencer_influenced_project\\topic_models\\git_repository\\ETM\\etm.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, optimizer, train_tokens, train_counts, test_1_tokens, test_1_counts, test_2_tokens, test_2_counts, vocab, ckpt)\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'optimization_params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epochs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m             cur_train_loss = self.train_single_epoch(epoch=epoch, optimizer=optimizer, train_tokens=train_tokens,\n\u001b[1;32m--> 393\u001b[1;33m                                                      train_counts=train_counts)\n\u001b[0m\u001b[0;32m    394\u001b[0m             \u001b[0mall_train_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_train_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m             val_ppl = self.evaluate(test_1_tokens=test_1_tokens, test_1_counts=test_1_counts,\n",
      "\u001b[1;32m~\\Documents\\Private\\IDC\\influencer_influenced_project\\topic_models\\git_repository\\ETM\\etm.py\u001b[0m in \u001b[0;36mtrain_single_epoch\u001b[1;34m(self, epoch, optimizer, train_tokens, train_counts)\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mrecon_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkld_theta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalized_data_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecon_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkld_theta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[0mtotal_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'optimization_params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clip'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "etm_model.fit(optimizer=optimizer, train_tokens=train_tokens, train_counts=train_counts,\n",
    "                      test_1_tokens=test_1_tokens, test_1_counts=test_1_counts, test_2_tokens=test_2_tokens,\n",
    "                      test_2_counts=test_2_counts, vocab=vocab, ckpt=ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction to new data using an existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_model_f_name = config_dict['data_prep_params']['saving_model_f_name'] + '.p'\n",
    "preprocess_obj = ArabicTwitterPreProcess.load_obj(f_path=config_dict['saving_models_path'][machine],\n",
    "                                                  f_name=preprocess_model_f_name)\n",
    "bow_new_docs_tokens, bow_new_docs_counts = preprocess_obj.transform(data_path=config_dict['raw_data_path'][machine])\n",
    "data_batch = data.get_batch(bow_new_docs_tokens, bow_new_docs_counts,\n",
    "                            torch.tensor(list(range(len(bow_new_docs_tokens)))), vocab_size, device)\n",
    "sums = data_batch.sum(1).unsqueeze(1)\n",
    "normalized_data_batch = data_batch / sums\n",
    "new_docs_prediction = etm_model.predict_proba(bow_tokens=normalized_data_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evluating an existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval option\n",
    "with open(ckpt, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print('Visualizing model quality before training...')\n",
    "etm_model.eval()\n",
    "etm_model.print_words_per_topic(words_amount=config_dict['evaluation_params']['num_words'],\n",
    "                                vocab=vocab, lang='en')\n",
    "with torch.no_grad():\n",
    "    # get document completion perplexities\n",
    "    test_ppl = model.evaluate(source='test', test_1_tokens=test_1_tokens, test_1_counts=test_1_counts,\n",
    "                              test_2_tokens=test_2_tokens, test_2_counts=test_2_counts,\n",
    "                              train_tokens=train_tokens, vocab=vocab, tc=config_dict['evaluation_params']['tc'],\n",
    "                              td=config_dict['evaluation_params']['td'])\n",
    "\n",
    "    # get most used topics\n",
    "    indices = torch.tensor(range(config_dict['num_docs_train']))\n",
    "    indices = torch.split(indices, config_dict['batch_size'])\n",
    "    thetaAvg = torch.zeros(1, config_dict['model_params']['num_topics']).to(device)\n",
    "    thetaWeightedAvg = torch.zeros(1, config_dict['model_params']['num_topics']).to(device)\n",
    "    cnt = 0\n",
    "    for idx, ind in enumerate(indices):\n",
    "        data_batch = data.get_batch(train_tokens, train_counts, ind, config_dict['vocab_size'], device)\n",
    "        sums = data_batch.sum(1).unsqueeze(1)\n",
    "        cnt += sums.sum(0).squeeze().cpu().numpy()\n",
    "        if config_dict['optimization_params']['bow_norm']:\n",
    "            normalized_data_batch = data_batch / sums\n",
    "        else:\n",
    "            normalized_data_batch = data_batch\n",
    "        theta, _ = model.get_theta(normalized_data_batch)\n",
    "        thetaAvg += theta.sum(0).unsqueeze(0) / config_dict['num_docs_train']\n",
    "        weighed_theta = sums * theta\n",
    "        thetaWeightedAvg += weighed_theta.sum(0).unsqueeze(0)\n",
    "        if idx % 100 == 0 and idx > 0:\n",
    "            print('batch: {}/{}'.format(idx, len(indices)))\n",
    "    thetaWeightedAvg = thetaWeightedAvg.squeeze().cpu().numpy() / cnt\n",
    "    print('\\nThe 10 most used topics are {}'.format(thetaWeightedAvg.argsort()[::-1][:10]))\n",
    "\n",
    "    # show topics\n",
    "    etm_model.print_words_per_topic(words_amount=config_dict['evaluation_params']['num_words'], vocab=vocab)\n",
    "    etm_model.print_words_per_topic(words_amount=config_dict['evaluation_params']['num_words'],\n",
    "                                    vocab=vocab, lang='en')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
